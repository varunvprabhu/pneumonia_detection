{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import glob, pylab, pandas as pd\n",
    "import pydicom, numpy as np\n",
    "import cv2\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import h5py\n",
    "import tables\n",
    "from decimal import Decimal\n",
    "import time\n",
    "import functools\n",
    "from functools import reduce\n",
    "import cv2\n",
    "import csv\n",
    "import imageio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this ensures the program can use all the gpu resources it can get\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "def weight_initializer(weight_input, output_channel_size, filter_size): \n",
    "    \n",
    "    _, rows, columns, input_channel_size = [i.value for i in weight_input.get_shape()]\n",
    "    \n",
    "    weight_shape = [filter_size,filter_size,input_channel_size,output_channel_size]\n",
    "\n",
    "    weight_output = tf.Variable(tf.contrib.layers.xavier_initializer(uniform = False)(weight_shape))\n",
    "        \n",
    "    return weight_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution block \n",
    "def conv2d(block_input, num_filters, filter_size = 1, stride_length = 1): \n",
    "    \n",
    "    init_weights = weight_initializer(block_input, num_filters, filter_size) \n",
    "    strides = [1,stride_length,stride_length,1]\n",
    "    block_output = tf.nn.conv2d(block_input,init_weights,strides,padding='VALID')\n",
    "    \n",
    "    return block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(block_input, num_filters, filter_size = 1, stride_length = 1): \n",
    "    \n",
    "    init_weights = weight_initializer(block_input, num_filters, filter_size) \n",
    "    strides = [1,stride_length,stride_length,1]\n",
    "    \n",
    "    block_output = tf.nn.conv2d(block_input,init_weights,strides,padding='VALID')\n",
    "    normalized = tf.contrib.layers.batch_norm(block_output, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(block_input, num_filters):\n",
    "    norm_1 = tf.contrib.layers.batch_norm(block_input, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    conv_1 = conv2d(norm_1, int(num_filters/2), 1, 1)\n",
    "    norm_2 = tf.contrib.layers.batch_norm(conv_1, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    pad = tf.pad(norm_2, np.array([[0,0],[1,1],[1,1],[0,0]]))\n",
    "    conv_2 = conv2d(pad, int(num_filters/2), 3, 1)\n",
    "    norm_3 = tf.contrib.layers.batch_norm(conv_2, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    conv_3 = conv2d(norm_3, int(num_filters), 1, 1)\n",
    "    \n",
    "    return conv_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_layer(block_input, num_filters):\n",
    "    \n",
    "    if (block_input.get_shape()[3] == num_filters):\n",
    "        return block_input\n",
    "    else:\n",
    "        conv = conv2d(block_input, num_filters,1,1)\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(block_input, num_filters):\n",
    "    conv = conv_block(block_input, num_filters)\n",
    "    skip = skip_layer(block_input, num_filters)\n",
    "    \n",
    "    return(tf.add_n([conv,skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourglass_unit(input_data, reduction_factor, num_filters):\n",
    "    up_1 = residual(input_data, num_filters)\n",
    "    low = tf.contrib.layers.max_pool2d(input_data, [2,2],[2,2], 'VALID')\n",
    "    low_1 = residual(low, num_filters)\n",
    "    \n",
    "    if reduction_factor > 0:\n",
    "        low_2 = hourglass_unit(low_1, reduction_factor - 1, num_filters)\n",
    "    else:\n",
    "        low_2 = residual(low_1, num_filters)\n",
    "    \n",
    "    low_3 = residual(low_2, num_filters)\n",
    "    up_sample = tf.image.resize_nearest_neighbor(low_3, tf.shape(low_3)[1:3]*2)\n",
    "    return tf.add_n([up_1, up_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourglass_model(input_data, num_blocks, num_filters, reduction_factor, train_model):\n",
    "    pad_1 = tf.pad(input_data, np.array([[0,0],[2,2],[2,2],[0,0]]))\n",
    "    conv_1 = conv2d(pad_1, 64,6,2)\n",
    "    res_1 = residual(conv_1, 128)\n",
    "    pool_1 = tf.contrib.layers.max_pool2d(res_1, [2,2], [2,2], padding= 'VALID')\n",
    "    res_2 = residual(pool_1, 128)\n",
    "    res_3 = residual(res_2, num_filters)\n",
    "    \n",
    "    x1 = [None] * num_blocks\n",
    "    x2 = [None] * num_blocks\n",
    "    x3 = [None] * num_blocks\n",
    "    x4 = [None] * num_blocks\n",
    "    x5 = [None] * num_blocks\n",
    "    x6 = [None] * num_blocks\n",
    "    sum_all = [None] * num_blocks\n",
    "    \n",
    "    x1[0] = hourglass_unit(res_3, reduction_factor, num_filters)\n",
    "    x2[0] = conv_bn_relu(x1[0], num_filters)\n",
    "    x3[0] = conv2d(x2[0], num_filters, 1, 1)\n",
    "    x4[0] = tf.layers.dropout(x3[0], rate = 0.1, training = train_model)\n",
    "    x5[0] = conv2d(x2[0], num_filters, 1, 1)\n",
    "    x6[0] = conv2d(x5[0], num_filters, 1, 1)\n",
    "    sum_all[0] = tf.add_n([x4[0], x6[0], res_3])\n",
    "    \n",
    "    for i in range(1, num_blocks - 1):\n",
    "        x1[i] = hourglass_unit(sum_all[i-1], reduction_factor, num_filters)\n",
    "        x2[i] = conv_bn_relu(x1[i], num_filters)\n",
    "        x3[i] = conv2d(x2[i], num_filters, 1, 1)\n",
    "        x4[i] = tf.layers.dropout(x3[i], rate = 0.1, training = train_model)\n",
    "        x5[i] = conv2d(x2[i], num_filters, 1, 1)\n",
    "        x6[i] = conv2d(x5[i], num_filters, 1, 1)\n",
    "        sum_all[i] = tf.add_n([x4[i], x6[i], sum_all[i-1]])\n",
    "    \n",
    "    x1[num_blocks - 1] = hourglass_unit(sum_all[num_blocks - 2], reduction_factor, num_filters)\n",
    "    x2[num_blocks - 1] = conv_bn_relu(x1[num_blocks - 1], num_filters)\n",
    "    x4[num_blocks - 1] = tf.layers.dropout(x2[num_blocks - 1], rate = 0.1, training = train_model)\n",
    "    x5[num_blocks - 1] = conv2d(x4[num_blocks - 1], 3, 1, 1)\n",
    "    final_output = tf.image.resize_nearest_neighbor(x5[num_blocks - 1], tf.shape(x5[num_blocks - 1])[1:3]*2)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL LOADING BLOCK -> The code block below is where the model weights are loaded. Select the model you want to load. The other model must be loaded after the intermediate csv file is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lung_x_input = tf.placeholder(tf.float32,shape=(batch_size,256,256,3),name='pose_img_ip')\n",
    "###################################################################################################################\n",
    "\n",
    "lung_x_input = lung_x_input/255.0\n",
    "\n",
    "hg_output = hourglass_model(lung_x_input, 4, 256, 3, False) # true while training, false during inference\n",
    "###################################################################################################################\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "restore_model = True\n",
    "\n",
    "#restore model weights. \n",
    "if(restore_model):\n",
    "    saver =  tf.train.Saver()  \n",
    "    saver.restore(sess,'../models/lungs_3_5') #change the model name here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERMEDIATE CSV BLOCK - After the model weights are loaded from the block above, you can use the code block below to generate the intermediate csv file. You can change the csv file name and the pixel threshold values below. \n",
    "\n",
    "for model lungs_4_2, use threshold value of 50 and csv file name of submission_28.csv\n",
    "for model lungs_3_5, use threshold value of 60 and csv file name of submission_50.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for generating the csv file\n",
    "\n",
    "df_test = pd.read_csv('../data/stage_1_sample_submission.csv')\n",
    "\n",
    "with open('../output/submission_28.csv', 'w', newline='') as csvfile: #CHANGE THE CSV FILE NAME HERE\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    for i in range(df_test.shape[0]): #df_test.shape[0]\n",
    "        patientId = df_test['patientId'][i]\n",
    "        dcm_file = '../data/stage_1_test_images/%s.dcm' % patientId\n",
    "        dcm_data = pydicom.read_file(dcm_file)\n",
    "        im = dcm_data.pixel_array\n",
    "        im = np.stack([im] * 3, axis=2)\n",
    "        original_img = im\n",
    "        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n",
    "        temp_content = im.reshape(1,256,256,3)\n",
    "\n",
    "        temp_outputs = sess.run([hg_output],feed_dict={lung_x_input:temp_content})\n",
    "        temp_outputs = np.asarray(temp_outputs)\n",
    "\n",
    "        gen_img = np.clip(temp_outputs[0,0,:,:,:], 0, 255).astype('uint8')\n",
    "        \n",
    "        gray = cv2.cvtColor(gen_img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        #submission_28.csv THRESHOLD VALUE 50 area val 256. score - 0.134. model used - lungs_4_2\n",
    "        #submission_50.csv THRESHOLD VALUE 60 area 256 score - 0.126 model used - lungs_3_5\n",
    "        \n",
    "        thresh_val = np.array([50]) #YOU CAN CHANGE THE PIXEL THRESHOLD VALUE HERE (CHANGE 50 TO 60 OR VICE VERSA).\n",
    "        selected_thresh_val = 0\n",
    "        val_is_good = False\n",
    "        \n",
    "        for j in range(thresh_val.shape[0]):\n",
    "            gray_2 = cv2.threshold(gray, thresh_val[j], 255, cv2.THRESH_BINARY)[1]\n",
    "            selected_thresh_val = thresh_val[j]\n",
    "            output = cv2.connectedComponentsWithStats(gray_2, 8, cv2.CV_32S)\n",
    "            num_labels = output[0]\n",
    "            stats = output[2]\n",
    "                \n",
    "        if num_labels > 1:\n",
    "            box_text = \"\"\n",
    "            for m in range(stats.shape[0]):\n",
    "                if m > 0 and stats[m,4] > 256:              \n",
    "                    box_text = box_text + \"1.0 \" + str(stats[m,0]*8) + \" \" + str(stats[m,1]*8) + \" \" + str(stats[m,2]*8) + \" \" + str(stats[m,3]*8) + \" \"\n",
    "\n",
    "            #print(box_text)\n",
    "            csvwriter.writerow([patientId, box_text])\n",
    "        else:\n",
    "            csvwriter.writerow([patientId, \"\"])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL CSV BLOCK -> To generate the final csv file for submission, I combine the two intermediate csv files generated from the two models. Generate the two intermediate files above by loading the two models separately. Then run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then simply combine the csv output of the models below to generate the final result.\n",
    "df_left = pd.read_csv('../output/submission_50.csv')\n",
    "df_right = pd.read_csv('../output/submission_28.csv')\n",
    "\n",
    "with open('../output/submission_combo_1.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    for i in range(df_left.shape[0]):\n",
    "        patientId = df_left['patientId'][i]\n",
    "        data_left = df_left['PredictionString'][i]\n",
    "        data_right = df_right['PredictionString'][i]\n",
    "                \n",
    "        #if data on left is more than right copy the whole data over\n",
    "        if not pd.isnull(data_right) and not pd.isnull(data_left):\n",
    "            len_right = len(data_right.split(\" \"))\n",
    "            len_left = len(data_left.split(\" \"))\n",
    "            \n",
    "            if len_left > len_right:\n",
    "                data_right = data_left\n",
    "            \n",
    "        #lets try - if data on left and no data on right, copy over\n",
    "        if pd.isnull(data_right) and not pd.isnull(data_left):\n",
    "            data_right = data_left   \n",
    "            \n",
    "        if pd.isnull(data_right):\n",
    "            data_right = \"\"\n",
    "            \n",
    "        csvwriter.writerow([patientId, data_right])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
